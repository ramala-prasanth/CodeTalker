{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramala-prasanth/CodeTalker/blob/main/Speech_Driven_3D_Facial_Animation_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmttp44lgqIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/miccunifi/ScanTalk.git &> /dev/null\n",
        "!pip install trimesh\n",
        "!pip install transformers\n",
        "!pip install git+https://github.com/MPI-IS/mesh.git\n",
        "!pip install pyrender\n",
        "!pip install robust_laplacian potpourri3d\n",
        "!pip install sacremoses\n",
        "!pip install transformers[deepspeed]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ-_PZjAfRcp",
        "outputId": "62681394-58e2-499d-827c-d684d06f05a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.5.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.26.4)\n",
            "Downloading trimesh-4.5.3-py3-none-any.whl (704 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/704.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.8/704.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trimesh\n",
            "Successfully installed trimesh-4.5.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Collecting git+https://github.com/MPI-IS/mesh.git\n",
            "  Cloning https://github.com/MPI-IS/mesh.git to /tmp/pip-req-build-kw9o66ya\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MPI-IS/mesh.git /tmp/pip-req-build-kw9o66ya\n",
            "  Resolved https://github.com/MPI-IS/mesh.git to commit 49e70425cf373ec5269917012bda2944215c5ccd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (4.10.0.84)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (11.0.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (3.1.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (6.0.2)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (24.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from psbody-mesh==0.4) (1.13.1)\n",
            "Building wheels for collected packages: psbody-mesh\n",
            "  Building wheel for psbody-mesh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psbody-mesh: filename=psbody_mesh-0.4-cp310-cp310-linux_x86_64.whl size=2321298 sha256=f3746e1319459dabcfb6560cfb8d499230b80a9cc01a0f794e5d7e18f272f643\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bd3yn5cu/wheels/50/e4/4f/3f1407f2c41d11f18944eba59d87157c4739efc0394e7cf300\n",
            "Successfully built psbody-mesh\n",
            "Installing collected packages: psbody-mesh\n",
            "Successfully installed psbody-mesh-0.4\n",
            "Collecting pyrender\n",
            "  Downloading pyrender-0.1.45-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting freetype-py (from pyrender)\n",
            "  Downloading freetype_py-2.5.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from pyrender) (2.36.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pyrender) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyrender) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from pyrender) (11.0.0)\n",
            "Collecting pyglet>=1.4.10 (from pyrender)\n",
            "  Downloading pyglet-2.0.20-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting PyOpenGL==3.1.0 (from pyrender)\n",
            "  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyrender) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyrender) (1.16.0)\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (from pyrender) (4.5.3)\n",
            "Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyglet-2.0.20-py3-none-any.whl (945 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m945.1/945.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading freetype_py-2.5.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: PyOpenGL\n",
            "  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745193 sha256=9401fba93adcb6e33cf7db2ca274f56baba83b60b70a0bb9ab52c5102a061693\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/3c/d2/1f9533f908d86176637521e533c6cdb2d4e48b59003b5c3f19\n",
            "Successfully built PyOpenGL\n",
            "Installing collected packages: PyOpenGL, pyglet, freetype-py, pyrender\n",
            "  Attempting uninstall: PyOpenGL\n",
            "    Found existing installation: PyOpenGL 3.1.7\n",
            "    Uninstalling PyOpenGL-3.1.7:\n",
            "      Successfully uninstalled PyOpenGL-3.1.7\n",
            "Successfully installed PyOpenGL-3.1.0 freetype-py-2.5.1 pyglet-2.0.20 pyrender-0.1.45\n",
            "Collecting robust_laplacian\n",
            "  Downloading robust_laplacian-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting potpourri3d\n",
            "  Downloading potpourri3d-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from robust_laplacian) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from potpourri3d) (1.13.1)\n",
            "Downloading robust_laplacian-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading potpourri3d-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (881 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.1/881.1 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: robust_laplacian, potpourri3d\n",
            "Successfully installed potpourri3d-1.1.0 robust_laplacian-1.0.0\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.6)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.1.1\n",
            "Requirement already satisfied: transformers[deepspeed] in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (4.66.6)\n",
            "Collecting deepspeed>=0.9.3 (from transformers[deepspeed])\n",
            "  Downloading deepspeed-0.16.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[deepspeed]) (1.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[deepspeed]) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[deepspeed]) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (0.8.0)\n",
            "Collecting hjson (from deepspeed>=0.9.3->transformers[deepspeed])\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (1.1.0)\n",
            "Collecting ninja (from deepspeed>=0.9.3->transformers[deepspeed])\n",
            "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed>=0.9.3->transformers[deepspeed]) (2.10.3)\n",
            "Collecting nvidia-ml-py (from deepspeed>=0.9.3->transformers[deepspeed])\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[deepspeed]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[deepspeed]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[deepspeed]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[deepspeed]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[deepspeed]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[deepspeed]) (2024.8.30)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed>=0.9.3->transformers[deepspeed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed>=0.9.3->transformers[deepspeed]) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0->transformers[deepspeed]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0->transformers[deepspeed]) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0->transformers[deepspeed]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0->transformers[deepspeed]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0->transformers[deepspeed]) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.1-py3-none-any.whl size=1543914 sha256=6147ecdec3b445e05f83fc4d7fe371c670bfa9785794c7e58ee20aa53a4c7855\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/a6/09/09e982334b832b202b1752644a6b314b922eb3897416846b7e\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.1 hjson-3.1.0 ninja-1.11.1.2 nvidia-ml-py-12.560.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1E1TLMf9ZicfHkxksg5qyFJknRqpoE7eh/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1fcHPqV0A-qMIu0f5rN1OxSPrfXMmBfM1/view?usp=sharing\n",
        "!gdown --id 1E1TLMf9ZicfHkxksg5qyFJknRqpoE7eh\n",
        "!gdown --id 1fcHPqV0A-qMIu0f5rN1OxSPrfXMmBfM1\n",
        "\n",
        "!mkdir -p /content/ScanTalk/src/results/\n",
        "!mv /content/scantalk_masked_velocity_loss.pth.tar /content/scantalk_mse.pth.tar /content/ScanTalk/src/results/."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj4jJ1gMbYCD",
        "outputId": "24946a29-feab-4099-d4d3-cee9bf358e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1E1TLMf9ZicfHkxksg5qyFJknRqpoE7eh\n",
            "From (redirected): https://drive.google.com/uc?id=1E1TLMf9ZicfHkxksg5qyFJknRqpoE7eh&confirm=t&uuid=c8422df9-de5e-41a0-b4e1-4f9c72fbe0c6\n",
            "To: /content/scantalk_masked_velocity_loss.pth.tar\n",
            "100% 1.10G/1.10G [00:22<00:00, 48.9MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fcHPqV0A-qMIu0f5rN1OxSPrfXMmBfM1\n",
            "From (redirected): https://drive.google.com/uc?id=1fcHPqV0A-qMIu0f5rN1OxSPrfXMmBfM1&confirm=t&uuid=58470d50-22f7-4c9e-a97f-a178cccf50d4\n",
            "To: /content/scantalk_mse.pth.tar\n",
            "100% 1.10G/1.10G [00:21<00:00, 51.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/ScanTalk/src/hubert/modeling_hubert.py\n",
        "# coding=utf-8\n",
        "# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" PyTorch Hubert model. \"\"\"\n",
        "\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from transformers.deepspeed import is_deepspeed_zero3_enabled\n",
        "\n",
        "from .activations import ACT2FN\n",
        "from .file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n",
        "from .modeling_outputs import BaseModelOutput, CausalLMOutput\n",
        "from .modeling_utils import PreTrainedModel\n",
        "from .utils import logging\n",
        "from .configuration_hubert import HubertConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CONFIG_FOR_DOC = \"HubertConfig\"\n",
        "\n",
        "HUBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"facebook/hubert-base-ls960\",\n",
        "    # See all Hubert models at https://huggingface.co/models?filter=hubert\n",
        "]\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n",
        "def _compute_mask_indices(\n",
        "    shape: Tuple[int, int],\n",
        "    mask_prob: float,\n",
        "    mask_length: int,\n",
        "    device: torch.device,\n",
        "    min_masks: int = 0,\n",
        ") -> torch.tensor:\n",
        "    \"\"\"\n",
        "    Computes random mask spans for a given shape. Used to implement `SpecAugment: A Simple Data Augmentation Method for\n",
        "    ASR <https://arxiv.org/abs/1904.08779>`__.\n",
        "\n",
        "    Args:\n",
        "        shape: the the shape for which to compute masks.\n",
        "            should be of size 2 where first element is batch size and 2nd is timesteps\n",
        "        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by\n",
        "            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.\n",
        "            however due to overlaps, the actual number will be smaller (unless no_overlap is True)\n",
        "        mask_length: size of the mask\n",
        "        min_masks: minimum number of masked spans\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size, sequence_length = shape\n",
        "\n",
        "    if mask_length < 1:\n",
        "        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n",
        "\n",
        "    if mask_length > sequence_length:\n",
        "        raise ValueError(\n",
        "            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length} and `sequence_length`: {sequence_length}`\"\n",
        "        )\n",
        "\n",
        "    # compute number of masked spans in batch\n",
        "    num_masked_spans = int(mask_prob * sequence_length / mask_length + torch.rand((1,)).item())\n",
        "    num_masked_spans = max(num_masked_spans, min_masks)\n",
        "\n",
        "    # make sure num masked indices <= sequence_length\n",
        "    if num_masked_spans * mask_length > sequence_length:\n",
        "        num_masked_spans = sequence_length // mask_length\n",
        "\n",
        "    # SpecAugment mask to fill\n",
        "    spec_aug_mask = torch.zeros((batch_size, sequence_length), device=device, dtype=torch.bool)\n",
        "\n",
        "    # uniform distribution to sample from, make sure that offset samples are < sequence_length\n",
        "    uniform_dist = torch.ones((batch_size, sequence_length - (mask_length - 1)), device=device)\n",
        "\n",
        "    # get random indices to mask\n",
        "    spec_aug_mask_idxs = torch.multinomial(uniform_dist, num_masked_spans)\n",
        "\n",
        "    # expand masked indices to masked spans\n",
        "    spec_aug_mask_idxs = (\n",
        "        spec_aug_mask_idxs.unsqueeze(dim=-1)\n",
        "        .expand((batch_size, num_masked_spans, mask_length))\n",
        "        .reshape(batch_size, num_masked_spans * mask_length)\n",
        "    )\n",
        "    offsets = (\n",
        "        torch.arange(mask_length, device=device)[None, None, :]\n",
        "        .expand((batch_size, num_masked_spans, mask_length))\n",
        "        .reshape(batch_size, num_masked_spans * mask_length)\n",
        "    )\n",
        "    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n",
        "\n",
        "    # scatter indices to mask\n",
        "    spec_aug_mask = spec_aug_mask.scatter(1, spec_aug_mask_idxs, True)\n",
        "\n",
        "    return spec_aug_mask\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->Hubert\n",
        "class HubertNoLayerNormConvLayer(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n",
        "        self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.in_conv_dim,\n",
        "            self.out_conv_dim,\n",
        "            kernel_size=config.conv_kernel[layer_id],\n",
        "            stride=config.conv_stride[layer_id],\n",
        "            bias=config.conv_bias,\n",
        "        )\n",
        "        self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.conv(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->Hubert\n",
        "class HubertLayerNormConvLayer(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n",
        "        self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.in_conv_dim,\n",
        "            self.out_conv_dim,\n",
        "            kernel_size=config.conv_kernel[layer_id],\n",
        "            stride=config.conv_stride[layer_id],\n",
        "            bias=config.conv_bias,\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n",
        "        self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.conv(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.transpose(-2, -1)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = hidden_states.transpose(-2, -1)\n",
        "\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->Hubert\n",
        "class HubertGroupNormConvLayer(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.in_conv_dim = config.conv_dim[layer_id] if layer_id > 0 else 1\n",
        "        self.out_conv_dim = config.conv_dim[layer_id]\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            self.in_conv_dim,\n",
        "            self.out_conv_dim,\n",
        "            kernel_size=config.conv_kernel[layer_id],\n",
        "            stride=config.conv_stride[layer_id],\n",
        "            bias=config.conv_bias,\n",
        "        )\n",
        "        self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.conv(hidden_states)\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->Hubert\n",
        "class HubertPositionalConvEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(\n",
        "            config.hidden_size,\n",
        "            config.hidden_size,\n",
        "            kernel_size=config.num_conv_pos_embeddings,\n",
        "            padding=config.num_conv_pos_embeddings // 2,\n",
        "            groups=config.num_conv_pos_embedding_groups,\n",
        "        )\n",
        "\n",
        "        if 1:\n",
        "            import deepspeed\n",
        "\n",
        "            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n",
        "                self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n",
        "            deepspeed.zero.register_external_parameter(self, self.conv.weight_v)\n",
        "            deepspeed.zero.register_external_parameter(self, self.conv.weight_g)\n",
        "        else:\n",
        "            self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n",
        "\n",
        "        self.padding = HubertSamePadLayer(config.num_conv_pos_embeddings)\n",
        "        self.activation = ACT2FN[config.feat_extract_activation]\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = hidden_states.transpose(1, 2)\n",
        "\n",
        "        hidden_states = self.conv(hidden_states)\n",
        "        hidden_states = self.padding(hidden_states)\n",
        "        hidden_states = self.activation(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states.transpose(1, 2)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->Hubert\n",
        "class HubertSamePadLayer(nn.Module):\n",
        "    def __init__(self, num_conv_pos_embeddings):\n",
        "        super().__init__()\n",
        "        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        if self.num_pad_remove > 0:\n",
        "            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureExtractor with Wav2Vec2->Hubert\n",
        "class HubertFeatureExtractor(nn.Module):\n",
        "    \"\"\"Construct the featurs from raw audio waveform\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        if config.feat_extract_norm == \"group\":\n",
        "            conv_layers = [HubertGroupNormConvLayer(config, layer_id=0)] + [\n",
        "                HubertNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
        "            ]\n",
        "        elif config.feat_extract_norm == \"layer\":\n",
        "            conv_layers = [HubertLayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
        "            )\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "\n",
        "    def _freeze_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        hidden_states = input_values[:, None]\n",
        "        for conv_layer in self.conv_layers:\n",
        "            hidden_states = conv_layer(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class HubertFeatureProjection(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)\n",
        "        self.projection = nn.Linear(config.conv_dim[-1], config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.feat_proj_dropout)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # non-projected hidden states are needed for quantization\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = self.projection(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Hubert\n",
        "class HubertAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == self.embed_dim\n",
        "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
        "                raise ValueError(\n",
        "                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            if layer_head_mask.size() != (self.num_heads,):\n",
        "                raise ValueError(\n",
        "                    f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
        "                )\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit awkward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to be reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
        "            )\n",
        "\n",
        "        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "        attn_output = attn_output.transpose(1, 2)\n",
        "        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->Hubert\n",
        "class HubertFeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.intermediate_dropout = nn.Dropout(config.activation_dropout)\n",
        "\n",
        "        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config.hidden_dropout)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.intermediate_dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        hidden_states = self.intermediate_dropout(hidden_states)\n",
        "\n",
        "        hidden_states = self.output_dense(hidden_states)\n",
        "        hidden_states = self.output_dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->Hubert\n",
        "class HubertEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = HubertAttention(\n",
        "            embed_dim=config.hidden_size,\n",
        "            num_heads=config.num_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=False,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.feed_forward = HubertFeedForward(config)\n",
        "        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n",
        "        attn_residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.attention(\n",
        "            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n",
        "        )\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = attn_residual + hidden_states\n",
        "\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = hidden_states + self.feed_forward(hidden_states)\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm with Wav2Vec2->Hubert\n",
        "class HubertEncoderLayerStableLayerNorm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = HubertAttention(\n",
        "            embed_dim=config.hidden_size,\n",
        "            num_heads=config.num_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=False,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.feed_forward = HubertFeedForward(config)\n",
        "        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n",
        "        attn_residual = hidden_states\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states, attn_weights, _ = self.attention(\n",
        "            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n",
        "        )\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = attn_residual + hidden_states\n",
        "        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder with Wav2Vec2->Hubert\n",
        "class HubertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.pos_conv_embed = HubertPositionalConvEmbedding(config)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
        "        self.layers = nn.ModuleList([HubertEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # make sure padded tokens output 0\n",
        "            hidden_states[~attention_mask] = 0.0\n",
        "\n",
        "            # extend attention_mask\n",
        "            attention_mask = (1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)) * -10000.0\n",
        "            attention_mask = attention_mask.expand(\n",
        "                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n",
        "            )\n",
        "\n",
        "        position_embeddings = self.pos_conv_embed(hidden_states)\n",
        "        hidden_states = hidden_states + position_embeddings\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        deepspeed_zero3_is_enabled = 1\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = np.random.uniform(0, 1)\n",
        "\n",
        "            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n",
        "            if not skip_the_layer or deepspeed_zero3_is_enabled:\n",
        "                # under deepspeed zero3 all gpus must run in sync\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "                    # create gradient checkpointing function\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = layer(\n",
        "                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n",
        "                    )\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if skip_the_layer:\n",
        "                layer_outputs = (None, None)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm with Wav2Vec2->Hubert\n",
        "class HubertEncoderStableLayerNorm(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.pos_conv_embed = HubertPositionalConvEmbedding(config)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [HubertEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # make sure padded tokens are not attended to\n",
        "            hidden_states[~attention_mask] = 0\n",
        "\n",
        "            # extend attention_mask\n",
        "            attention_mask = (1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)) * -10000.0\n",
        "            attention_mask = attention_mask.expand(\n",
        "                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n",
        "            )\n",
        "\n",
        "        position_embeddings = self.pos_conv_embed(hidden_states)\n",
        "        hidden_states = hidden_states + position_embeddings\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        deepspeed_zero3_is_enabled = 1\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = np.random.uniform(0, 1)\n",
        "\n",
        "            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n",
        "            if not skip_the_layer or deepspeed_zero3_is_enabled:\n",
        "                # under deepspeed zero3 all gpus must run in sync\n",
        "                # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "                    # create gradient checkpointing function\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = layer(\n",
        "                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n",
        "                    )\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if skip_the_layer:\n",
        "                layer_outputs = (None, None)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class HubertPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = HubertConfig\n",
        "    base_model_prefix = \"hubert\"\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, nn.Conv1d):\n",
        "            if 1:\n",
        "                import deepspeed\n",
        "\n",
        "                if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n",
        "                    with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n",
        "                        nn.init.kaiming_normal_(module.weight.data)\n",
        "                else:\n",
        "                    with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n",
        "                        nn.init.kaiming_normal_(module.weight.data)\n",
        "            else:\n",
        "                nn.init.kaiming_normal_(module.weight.data)\n",
        "\n",
        "        if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n",
        "        \"\"\"\n",
        "        Computes the output length of the convolutional layers\n",
        "        \"\"\"\n",
        "\n",
        "        def _conv_out_length(input_length, kernel_size, stride):\n",
        "            # 1D convolutional layer output length formula taken\n",
        "            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
        "            return (input_length - kernel_size) // stride + 1\n",
        "\n",
        "        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n",
        "            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n",
        "\n",
        "        return input_lengths\n",
        "\n",
        "\n",
        "HUBERT_START_DOCSTRING = r\"\"\"\n",
        "    Hubert was proposed in `HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\n",
        "    <https://arxiv.org/abs/2106.07447>`__ by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\n",
        "    Ruslan Salakhutdinov, Abdelrahman Mohamed.\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving etc.).\n",
        "\n",
        "    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ sub-class. Use\n",
        "    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n",
        "    behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.HubertConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "HUBERT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_values (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n",
        "            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n",
        "            soundfile`). To prepare the array into `input_values`, the :class:`~transformers.Wav2Vec2Processor` should\n",
        "            be used for padding and conversion into a tensor of type `torch.FloatTensor`. See\n",
        "            :meth:`transformers.Wav2Vec2Processor.__call__` for details.\n",
        "        attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "\n",
        "            .. warning::\n",
        "                :obj:`attention_mask` should only be passed if the corresponding processor has\n",
        "                ``config.return_attention_mask == True``. For all models whose processor has\n",
        "                ``config.return_attention_mask == False``, such as `hubert-base\n",
        "                <https://huggingface.co/facebook/hubert-base-ls960>`__, :obj:`attention_mask` should **not** be passed\n",
        "                to avoid degraded performance when doing batched inference. For such models :obj:`input_values` should\n",
        "                simply be padded with 0 and passed without :obj:`attention_mask`. Be aware that these models also yield\n",
        "                slightly different results depending on whether :obj:`input_values` is padded or not.\n",
        "\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    HUBERT_START_DOCSTRING,\n",
        ")\n",
        "\n",
        "def linear_interpolation(features, input_fps, output_fps, output_len=None):\n",
        "    features = features.transpose(1, 2)\n",
        "    seq_len = features.shape[2] / float(input_fps)\n",
        "    if output_len is None:\n",
        "        output_len = int(seq_len * output_fps)\n",
        "    output_features = F.interpolate(features,size=output_len,align_corners=True,mode='linear')\n",
        "    return output_features.transpose(1, 2)\n",
        "\n",
        "class HubertModel(HubertPreTrainedModel):\n",
        "    def __init__(self, config: HubertConfig):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.feature_extractor = HubertFeatureExtractor(config)\n",
        "        self.feature_projection = HubertFeatureProjection(config)\n",
        "\n",
        "        self.masked_spec_embed = nn.Parameter(torch.FloatTensor(config.hidden_size).uniform_())\n",
        "\n",
        "        if config.do_stable_layer_norm:\n",
        "            self.encoder = HubertEncoderStableLayerNorm(config)\n",
        "        else:\n",
        "            self.encoder = HubertEncoder(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _mask_hidden_states(\n",
        "        self, hidden_states: torch.FloatTensor, mask_time_indices: Optional[torch.FloatTensor] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Masks extracted features along time axis and/or along feature axis according to `SpecAugment\n",
        "        <https://arxiv.org/abs/1904.08779>`__ .\n",
        "        \"\"\"\n",
        "\n",
        "        # `config.apply_spec_augment` can set masking to False\n",
        "        if not getattr(self.config, \"apply_spec_augment\", True):\n",
        "            return hidden_states\n",
        "\n",
        "        if mask_time_indices is not None:\n",
        "            # apply SpecAugment along time axis with given mask_time_indices\n",
        "            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n",
        "        elif self.config.mask_time_prob > 0 and self.training:\n",
        "            # generate indices & apply SpecAugment along time axis\n",
        "            batch_size, sequence_length, hidden_size = hidden_states.size()\n",
        "\n",
        "            mask_time_indices = _compute_mask_indices(\n",
        "                (batch_size, sequence_length),\n",
        "                mask_prob=self.config.mask_time_prob,\n",
        "                mask_length=self.config.mask_time_length,\n",
        "                device=hidden_states.device,\n",
        "                min_masks=2,\n",
        "            )\n",
        "            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n",
        "\n",
        "        if self.config.mask_feature_prob > 0 and self.training:\n",
        "            # generate indices & apply SpecAugment along feature axis\n",
        "            mask_feature_indices = _compute_mask_indices(\n",
        "                (batch_size, hidden_size),\n",
        "                mask_prob=self.config.mask_feature_prob,\n",
        "                mask_length=self.config.mask_feature_length,\n",
        "                device=hidden_states.device,\n",
        "            )\n",
        "            hidden_states[mask_feature_indices[:, None].expand(-1, sequence_length, -1)] = 0\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values,\n",
        "        dataset,\n",
        "        attention_mask=None,\n",
        "        mask_time_indices=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        frame_num=None\n",
        "\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import Wav2Vec2Processor, HubertModel\n",
        "            >>> from datasets import load_dataset\n",
        "            >>> import soundfile as sf\n",
        "\n",
        "            >>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "            >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "\n",
        "            >>> def map_to_array(batch):\n",
        "            ...     speech, _ = sf.read(batch[\"file\"])\n",
        "            ...     batch[\"speech\"] = speech\n",
        "            ...     return batch\n",
        "\n",
        "            >>> ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "            >>> ds = ds.map(map_to_array)\n",
        "\n",
        "            >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "            >>> hidden_states = model(input_values).last_hidden_state\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        input_values = torch.squeeze(input_values)\n",
        "        input_values = torch.unsqueeze(input_values,0)\n",
        "\n",
        "        extract_features = self.feature_extractor(input_values)\n",
        "        extract_features = extract_features.transpose(1, 2)\n",
        "\n",
        "        if dataset == 'vocaset' or dataset == 'multiface':\n",
        "            fps = 30\n",
        "        if dataset == 'BIWI':\n",
        "            fps = 25\n",
        "\n",
        "        extract_features = linear_interpolation(extract_features, 50, fps, output_len=frame_num)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # compute real output lengths according to convolution formula\n",
        "            output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
        "\n",
        "            attention_mask = torch.zeros(\n",
        "                extract_features.shape[:2], dtype=extract_features.dtype, device=extract_features.device\n",
        "            )\n",
        "\n",
        "            # these two operations makes sure that all values\n",
        "            # before the output lengths indices are attended to\n",
        "            attention_mask[\n",
        "                (torch.arange(attention_mask.shape[0], device=extract_features.device), output_lengths - 1)\n",
        "            ] = 1\n",
        "            attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n",
        "\n",
        "        hidden_states = self.feature_projection(extract_features)\n",
        "\n",
        "        if mask_time_indices is not None:  # apply SpecAugment along time axis with given indices\n",
        "            hidden_states[mask_time_indices] = self.masked_spec_embed.to(hidden_states.dtype)\n",
        "\n",
        "        hidden_states = self._mask_hidden_states(hidden_states)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = encoder_outputs[0]\n",
        "\n",
        "        if not return_dict:\n",
        "            return (hidden_states,) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC). \"\"\",\n",
        "    HUBERT_START_DOCSTRING,\n",
        ")\n",
        "class HubertForCTC(HubertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.hubert = HubertModel(config)\n",
        "        self.dropout = nn.Dropout(config.final_dropout)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        \"\"\"\n",
        "        Calling this function will disable the gradient computation for the feature extractor so that its parameter\n",
        "        will not be updated during training.\n",
        "        \"\"\"\n",
        "        self.hubert.feature_extractor._freeze_parameters()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)\n",
        "    @replace_return_docstrings(output_type=CausalLMOutput, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_values,\n",
        "        attention_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_length)`, `optional`):\n",
        "            Labels for connectionist temporal classification. Note that ``target_length`` has to be smaller or equal to\n",
        "            the sequence length of the output logits. Indices are selected in ``[-100, 0, ..., config.vocab_size -\n",
        "            1]``. All labels set to ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ...,\n",
        "            config.vocab_size - 1]``.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> import torch\n",
        "            >>> from transformers import Wav2Vec2Processor, HubertForCTC\n",
        "            >>> from datasets import load_dataset\n",
        "            >>> import soundfile as sf\n",
        "\n",
        "            >>> processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "            >>> model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
        "\n",
        "            >>> def map_to_array(batch):\n",
        "            ...     speech, _ = sf.read(batch[\"file\"])\n",
        "            ...     batch[\"speech\"] = speech\n",
        "            ...     return batch\n",
        "\n",
        "            >>> ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "            >>> ds = ds.map(map_to_array)\n",
        "\n",
        "            >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n",
        "            >>> logits = model(input_values).logits\n",
        "            >>> predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            >>> transcription = processor.decode(predicted_ids[0])\n",
        "\n",
        "            >>> # compute loss\n",
        "            >>> target_transcription = \"A MAN SAID TO THE UNIVERSE SIR I EXIST\"\n",
        "\n",
        "            >>> # wrap processor as target processor to encode labels\n",
        "            >>> with processor.as_target_processor():\n",
        "            ...     labels = processor(target_transcription, return_tensors=\"pt\").input_ids\n",
        "\n",
        "            >>> loss = model(input_values, labels=labels).loss\n",
        "        \"\"\"\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.hubert(\n",
        "            input_values,\n",
        "            attention_mask=attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "\n",
        "            # retrieve loss input_lengths from attention_mask\n",
        "            attention_mask = (\n",
        "                attention_mask if attention_mask is not None else torch.ones_like(input_values, dtype=torch.long)\n",
        "            )\n",
        "            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
        "\n",
        "            # assuming that padded tokens are filled with -100\n",
        "            # when not being attended to\n",
        "            labels_mask = labels >= 0\n",
        "            target_lengths = labels_mask.sum(-1)\n",
        "            flattened_targets = labels.masked_select(labels_mask)\n",
        "\n",
        "            # ctc_loss doesn't support fp16\n",
        "            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
        "\n",
        "            with torch.backends.cudnn.flags(enabled=False):\n",
        "                loss = nn.functional.ctc_loss(\n",
        "                    log_probs,\n",
        "                    flattened_targets,\n",
        "                    input_lengths,\n",
        "                    target_lengths,\n",
        "                    blank=self.config.pad_token_id,\n",
        "                    reduction=self.config.ctc_loss_reduction,\n",
        "                    zero_infinity=self.config.ctc_zero_infinity,\n",
        "                )\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutput(\n",
        "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cO0Wpgn881W",
        "outputId": "546253ca-e866-406d-8f89-7cc3e83a75e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/ScanTalk/src/hubert/modeling_hubert.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ScanTalk\n",
        "!export PYTHONPATH=/content/ScanTalk:$PYTHONPATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z84-ZOEi8L11",
        "outputId": "c570246a-6bef-402d-ed29-79ab638476df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ScanTalk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ScanTalk/src\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyj28zrlcZEF",
        "outputId": "f73750c0-e8f1-4c52-a45d-5e70f0fea81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ScanTalk/src\n",
            "compute_metrics.py  \u001b[0m\u001b[01;34mexamples\u001b[0m/  \u001b[01;34mmodel\u001b[0m/    scantalk_dataloader.py  scantalk_train.py\n",
            "demo.py             \u001b[01;34mhubert\u001b[0m/    \u001b[01;34mresults\u001b[0m/  scantalk_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import demo  # Ensure demo.py is in the correct location\n",
        "import os\n",
        "\n",
        "def generate_video(audio_path='./examples/photo.wav',\n",
        "                   actor_file='./examples/thanos.ply',\n",
        "                   save_path='./Demo',\n",
        "                   model_path='./results/scantalk_masked_velocity_loss.pth.tar',\n",
        "                   video_name='demo.mp4',\n",
        "                   fps=30,\n",
        "                   device='cuda'):\n",
        "    # Create argument namespace\n",
        "    args = argparse.Namespace(\n",
        "        device=device,\n",
        "        save_path=save_path,\n",
        "        audio=audio_path,\n",
        "        actor_file=actor_file,\n",
        "        model_path=model_path,\n",
        "        video_name=video_name,\n",
        "        fps=fps,\n",
        "        latent_channels=32,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        lstm_layers=3\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "        os.mkdir(os.path.join(save_path, 'Meshes'))\n",
        "        os.mkdir(os.path.join(save_path, 'Images'))\n",
        "\n",
        "    # Call the main function from demo.py\n",
        "    demo.generate_meshes(args)\n",
        "\n",
        "    print('Starting Video Generation')\n",
        "    demo.generate_mesh_video(save_path,\n",
        "                        args.video_name,\n",
        "                        os.path.join(save_path, 'Meshes'),\n",
        "                        args.fps,\n",
        "                        args.audio)\n",
        "\n",
        "    return args.save_path + '/' + args.video_name"
      ],
      "metadata": {
        "id": "GnVAqrw65Ey4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "gnXBlzYTgYGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_video_path = generate_video(audio_path ='/content/ScanTalk/src/examples/photo.wav',\n",
        "                                   actor_file ='/content/ScanTalk/src/examples/FLAME_sample.ply',\n",
        "                                   model_path = '/content/ScanTalk/src/results/scantalk_masked_velocity_loss.pth.tar',\n",
        "                                   save_path='./output',\n",
        "                                   video_name='demo.mp4',\n",
        "                                   fps=30,\n",
        "                                   device='cuda'\n",
        "                                   )\n",
        "print(\"Video saved at:\", output_video_path)"
      ],
      "metadata": {
        "id": "W8Ukj9R2dWPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def play_video(video_path):\n",
        "    video = open(video_path, 'rb').read()\n",
        "    video_encoded = base64.b64encode(video).decode('utf-8')\n",
        "    return HTML(data='''<video width=\"640\" height=\"480\" controls>\n",
        "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "                        </video>'''.format(video_encoded))\n",
        "\n",
        "play_video(output_video_path)"
      ],
      "metadata": {
        "id": "viD8vvcz9_Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "Audio('/content/ScanTalk/src/examples/photo.wav')"
      ],
      "metadata": {
        "id": "vK5pLVWZ9_bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Interface"
      ],
      "metadata": {
        "id": "lJ1bmJvHgRee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit==1.32.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q3o0eTo0gMXY",
        "outputId": "fe90f176-5f11-4188-fa13-21038ce478fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit==1.32.0\n",
            "  Downloading streamlit-1.32.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (1.26.4)\n",
            "Collecting packaging<24,>=16.8 (from streamlit==1.32.0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (2.2.2)\n",
            "Collecting pillow<11,>=7.1.0 (from streamlit==1.32.0)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (13.9.4)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit==1.32.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.32.0)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.32.0) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit==1.32.0)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.32.0) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.32.0) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.32.0) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit==1.32.0) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.32.0) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit==1.32.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit==1.32.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit==1.32.0) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit==1.32.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit==1.32.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit==1.32.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit==1.32.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit==1.32.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit==1.32.0) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.32.0) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit==1.32.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.32.0) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.32.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.32.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.32.0) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.32.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit==1.32.0) (1.16.0)\n",
            "Downloading streamlit-1.32.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, tenacity, pillow, packaging, pydeck, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "Successfully installed packaging-23.2 pillow-10.4.0 pydeck-0.9.1 streamlit-1.32.0 tenacity-8.5.0 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "738b49cd71ea48ada0c1bde409983289"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ScanTalk/src\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vua_ngOhP1zb",
        "outputId": "d5b89628-4c56-491a-fe9f-e9c0a5473b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ScanTalk/src\n",
            "compute_metrics.py  \u001b[0m\u001b[01;34mexamples\u001b[0m/  \u001b[01;34mmodel\u001b[0m/        \u001b[01;34mresults\u001b[0m/                scantalk_test.py\n",
            "demo.py             \u001b[01;34mhubert\u001b[0m/    \u001b[01;34m__pycache__\u001b[0m/  scantalk_dataloader.py  scantalk_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get install libosmesa6\n",
        "# !export PYOPENGL_PLATFORM=osmesa"
      ],
      "metadata": {
        "id": "DUIa05EShM3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import argparse\n",
        "import demo  # Ensure demo.py is in the correct location\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "# Define the generate_video function as provided\n",
        "def generate_video(audio_path='./examples/photo.wav',\n",
        "                   actor_file='./examples/thanos.ply',\n",
        "                   save_path='./Demo',\n",
        "                   model_path='./results/scantalk_masked_velocity_loss.pth.tar',\n",
        "                   video_name='demo.mp4',\n",
        "                   fps=30,\n",
        "                   device='cuda'):\n",
        "    # Create argument namespace\n",
        "    args = argparse.Namespace(\n",
        "        device=device,\n",
        "        save_path=save_path,\n",
        "        audio=audio_path,\n",
        "        actor_file=actor_file,\n",
        "        model_path=model_path,\n",
        "        video_name=video_name,\n",
        "        fps=fps,\n",
        "        latent_channels=32,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        lstm_layers=3\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "        os.mkdir(os.path.join(save_path, 'Meshes'))\n",
        "        os.mkdir(os.path.join(save_path, 'Images'))\n",
        "\n",
        "    # Call the main function from demo.py\n",
        "    demo.generate_meshes(args)\n",
        "\n",
        "    print('Starting Video Generation')\n",
        "    demo.generate_mesh_video(save_path,\n",
        "                        args.video_name,\n",
        "                        os.path.join(save_path, 'Meshes'),\n",
        "                        args.fps,\n",
        "                        args.audio)\n",
        "\n",
        "    return os.path.join(args.save_path, args.video_name)\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Speech-Driven 3D Facial Animation App\")\n",
        "\n",
        "# Upload audio file\n",
        "uploaded_audio = st.file_uploader(\"Upload an audio file\", type=[\"wav\"])\n",
        "\n",
        "# Set actor file path (you can allow upload if needed)\n",
        "actor_file = '/content/ScanTalk/src/examples/FLAME_sample.ply'  # Update the path as needed\n",
        "\n",
        "# Set model path\n",
        "model_path = '/content/ScanTalk/src/results/scantalk_masked_velocity_loss.pth.tar'  # Update the path as needed\n",
        "\n",
        "# Set save path and video name\n",
        "save_path = './output'\n",
        "video_name = 'demo.mp4'\n",
        "\n",
        "# Create a generate button\n",
        "if st.button(\"Generate Video\"):\n",
        "    if uploaded_audio is not None:\n",
        "        # Save the uploaded audio to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as temp_audio:\n",
        "            temp_audio.write(uploaded_audio.read())\n",
        "            audio_path = temp_audio.name\n",
        "\n",
        "        # Generate video with a spinner\n",
        "        with st.spinner(\"Generating video...\"):\n",
        "            output_video_path = generate_video(\n",
        "                audio_path=audio_path,\n",
        "                actor_file=actor_file,\n",
        "                model_path=model_path,\n",
        "                save_path=save_path,\n",
        "                video_name=video_name,\n",
        "                fps=30,\n",
        "                device='cuda'\n",
        "            )\n",
        "\n",
        "        # Display the video\n",
        "        st.video(output_video_path)\n",
        "\n",
        "        # Clean up the temporary audio file\n",
        "        os.remove(audio_path)\n",
        "    else:\n",
        "        st.warning(\"Please upload an audio file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjeQAklzgMbO",
        "outputId": "aa91474a-d3d4-4e2c-af30-147246046a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V06L-97vgMeh",
        "outputId": "f8fc382b-a21e-41dd-9486-fb5ac5868b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# replace the key below with your ngrok key.\n",
        "ngrok_key = \"\"\n",
        "port = 8501\n",
        "\n",
        "ngrok.set_auth_token(ngrok_key)\n",
        "ngrok.connect(port).public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jQtODo1Kg4Mv",
        "outputId": "c61f1ba8-4175-44a4-fd46-4287b2add55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://a5cd-34-87-162-129.ngrok-free.app'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf logs.txt && streamlit run app.py &>/content/logs.txt"
      ],
      "metadata": {
        "id": "NMVal15Xg61k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}